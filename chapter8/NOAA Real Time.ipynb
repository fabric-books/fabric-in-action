{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a836b5-7390-4af4-a844-fecfee2e252c",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Data & Innovation Day - Realtime - Azure AI Services and Fabric demonstration\n",
    "\n",
    "The NOAA’s Center for Operational Oceanographic Products and Services (CO-OPS) provides real-time information on tides, water levels, currents, and other coastal oceanographic and meteorological data. This data serves a crucial purpose in aiding the maritime transportation industry by ensuring safe and efficient navigation through waterways and ports. In addition, current trends and predictions play a significant role in assisting individuals in preparing for potential flooding due to storms, tsunamis, and sea level fluctuations.\n",
    "\n",
    "![A blue and black rectangle with a white cross  Description automatically generated](https://dataplatformblogcdn.azureedge.net/wp-content/uploads/2023/07/a-blue-and-black-rectangle-with-a-white-cross-des.png)\n",
    "\n",
    "https://blog.fabric.microsoft.com/en-US/blog/microsoft-fabric-event-streams-generating-real-time-insights-with-python-kql-and-powerbi/\n",
    "\n",
    "https://tidesandcurrents.noaa.gov/map/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49e672e1-85c4-4fa7-a07c-b0009a56eea2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-10-28T10:00:08.0858099Z",
       "execution_start_time": "2024-10-28T09:59:42.2529346Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "a994a9c5-79c4-4f38-9172-4e70d6f65117",
       "queued_time": "2024-10-28T09:59:33.1650659Z",
       "session_id": "ff194b8d-2ac9-4e56-af8b-a6d79b20c670",
       "session_start_time": "2024-10-28T09:59:33.3398389Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        3,
        4,
        5,
        6,
        7,
        8,
        9,
        10
       ]
      },
      "text/plain": [
       "StatementMeta(, ff194b8d-2ac9-4e56-af8b-a6d79b20c670, 10, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-eventhub\n",
      "  Downloading azure_eventhub-5.12.2-py3-none-any.whl.metadata (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.3/69.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: azure-core>=1.27.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-eventhub) (1.30.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-eventhub) (4.9.0)\n",
      "Requirement already satisfied: requests>=2.21.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-core>=1.27.0->azure-eventhub) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-core>=1.27.0->azure-eventhub) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (2024.2.2)\n",
      "Downloading azure_eventhub-5.12.2-py3-none-any.whl (326 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.6/326.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: azure-eventhub\n",
      "Successfully installed azure-eventhub-5.12.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: azure-identity in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (1.15.0)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.23.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-identity) (1.30.2)\n",
      "Requirement already satisfied: cryptography>=2.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-identity) (42.0.2)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.24.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-identity) (1.25.0)\n",
      "Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-identity) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.21.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-core<2.0.0,>=1.23.0->azure-identity) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-core<2.0.0,>=1.23.0->azure-identity) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-core<2.0.0,>=1.23.0->azure-identity) (4.9.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from cryptography>=2.5->azure-identity) (1.16.0)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal<2.0.0,>=1.24.0->azure-identity) (2.4.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity) (2.3.0)\n",
      "Requirement already satisfied: pycparser in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.23.0->azure-identity) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.23.0->azure-identity) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.23.0->azure-identity) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.23.0->azure-identity) (2024.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: aiohttp in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (3.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp) (1.9.3)\n",
      "Requirement already satisfied: idna>=2.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp) (3.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Warning: PySpark kernel has been restarted to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Welcome to your new notebook\n",
    "# Type here in the cell editor to add code!\n",
    "%pip install azure-eventhub\n",
    "%pip install azure-identity\n",
    "%pip install aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b818a-1641-427d-b687-9654a2d686d4",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**Collecting Coastal Water Level and Atmospheric Data with Python**\n",
    "\n",
    "NOAA provides real-time water level information that is updated every 6 minutes. You can use HTTP Get API call to fetch water level and location information for a specific station. This API takes a StationID (full list is here) as a required input and some other optional parameters. \n",
    "\n",
    "```xml\n",
    "<data>\n",
    "<metadata id=\"8594900\" name=\"Washington\" lat=\"38.8733\" lon=\"-77.0217\"/>\n",
    "<observations>\n",
    "<wl t=\"2023-07-09 19:30\" v=\"8.232\" s=\"0.023\" f=\"1,0,0,0\" q=\"p\"/>\n",
    "</observations>\n",
    "</data>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01beed6-086a-4db8-bc71-508fb1a5513a",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**Create a request payload, issue the HTTP call and retrieve the response from NOAA into a Python function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b4608b-69de-438d-b357-efed1a7d1225",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-10-28T10:00:45.5811472Z",
       "execution_start_time": "2024-10-28T10:00:45.3532978Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "1b93f4f4-8246-4964-9d26-99dd3cf57029",
       "queued_time": "2024-10-28T10:00:41.8431284Z",
       "session_id": "ff194b8d-2ac9-4e56-af8b-a6d79b20c670",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 12,
       "statement_ids": [
        12
       ]
      },
      "text/plain": [
       "StatementMeta(, ff194b8d-2ac9-4e56-af8b-a6d79b20c670, 12, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "# Fetch water level data from tidesandcurrents.noaa.gov\n",
    "def fetch_water_level(station_id):\n",
    "    url = \"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter\"\n",
    "    # Collect water level for one station\n",
    "    payload = {\n",
    "        \"product\": \"water_level\",\n",
    "        \"application\": \"Fabric EventStream\",  # Replace with your application name\n",
    "        \"datum\": \"STND\",\n",
    "        \"station\": station_id,  # Set station ID\n",
    "        \"time_zone\": \"gmt\",\n",
    "        \"units\": \"english\",  # Use \"english\" for feet, \"metric\" for meters\n",
    "        \"format\": \"json\",\n",
    "        \"date\": \"latest\",\n",
    "    }\n",
    "    response = requests.get(url, params=payload)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a08f32-edb4-4bb6-9c17-6135cbc56923",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**Unit test of API Call**\n",
    "\n",
    "```javascript\n",
    "{'Current_time': '2024-01-12 08:20:25', 'Water Level Value': '8.937', 'Station Latitude': '47.5617', 'Station Longitude': '-122.6230', 'Station': 'Water Station'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "021809a7-1926-46f5-bba6-8bfb88e31bdd",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-10-28T10:00:50.6895024Z",
       "execution_start_time": "2024-10-28T10:00:49.850824Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "d4ad29c9-1571-429f-acc9-4f8a96eeea00",
       "queued_time": "2024-10-28T10:00:49.4770841Z",
       "session_id": "ff194b8d-2ac9-4e56-af8b-a6d79b20c670",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 13,
       "statement_ids": [
        13
       ]
      },
      "text/plain": [
       "StatementMeta(, ff194b8d-2ac9-4e56-af8b-a6d79b20c670, 13, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Current_time': '2024-10-28 10:00:50', 'Water Level Value': '20.371', 'Station Latitude': '47.5617', 'Station Longitude': '-122.6230', 'Station': 'Water Station'}\n"
     ]
    }
   ],
   "source": [
    "stationID = \"9445958\"  # Sample Bremerton, WA [9445958]\n",
    "station_data = fetch_water_level(stationID)\n",
    "\n",
    "if station_data is not None:\n",
    "                # Fetch station location\n",
    "                data = station_data[\"data\"]\n",
    "                if len(data) > 0:\n",
    "                    water_level = data[0][\"v\"]\n",
    "                station_latitude = station_data[\"metadata\"][\"lat\"]\n",
    "                station_longitude = station_data[\"metadata\"][\"lon\"]\n",
    "                # Create a data point with the time and value\n",
    "                current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                # Generate the data structure\n",
    "                water_point = {\n",
    "                    \"Current_time\": current_time,\n",
    "                    \"Water Level Value\": water_level,\n",
    "                    \"Station Latitude\": station_latitude,\n",
    "                    \"Station Longitude\": station_longitude,\n",
    "                    \"Station\": \"Water Station\",\n",
    "                }\n",
    "print(water_point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549ae83c-f48b-4137-a4a0-ee2b7706a820",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**Ingesting Data into Fabric Eventstreams (using Python)**\n",
    "\n",
    "Now that we have a well-tested Python code to retrieve all the data we need from its source, let us see how we can package the same in a ‘EventData’ structure and send it on to a Fabric Eventstream. For now, we will focus on the Python code but in a subsequent, we will show how to connect the Python code to a Eventstream.\n",
    "\n",
    "The above Python code acts as a Eventstream Producer/client and publishes water level data from NOAA every **6 minutes** to it. This is the interval at which NOAA refreshes its data. Refreshing at a higher frequency would be redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f1b8ca-f8e0-44c6-9171-8309f3ba32a5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-10-28T10:00:54.4060875Z",
       "execution_start_time": "2024-10-28T10:00:54.1694208Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "519f61d6-7dbc-4eb0-bc04-62b78bb1def6",
       "queued_time": "2024-10-28T10:00:53.8221822Z",
       "session_id": "ff194b8d-2ac9-4e56-af8b-a6d79b20c670",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 14,
       "statement_ids": [
        14
       ]
      },
      "text/plain": [
       "StatementMeta(, ff194b8d-2ac9-4e56-af8b-a6d79b20c670, 14, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# code for fabric event stream\n",
    "EVENTSTREAM_CONNECTION_STR = \"\"\n",
    "EVENTSTREAM_NAME = \"es_3f228009-e282-4216-852a-a4bb12080b44\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1839d2f2-9180-4c0c-b4cf-19dc612ea655",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": "2024-10-28T10:00:58.1600978Z",
       "livy_statement_state": "running",
       "normalized_state": "running",
       "parent_msg_id": "a3cfa0c0-9251-478f-ada7-c38474366e51",
       "queued_time": "2024-10-28T10:00:57.7721443Z",
       "session_id": "ff194b8d-2ac9-4e56-af8b-a6d79b20c670",
       "session_start_time": null,
       "spark_pool": null,
       "state": "submitted",
       "statement_id": 15,
       "statement_ids": [
        15
       ]
      },
      "text/plain": [
       "StatementMeta(, ff194b8d-2ac9-4e56-af8b-a6d79b20c670, 15, Submitted, Running, Running)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from azure.eventhub import EventData\n",
    "from azure.eventhub.aio import EventHubProducerClient\n",
    "\n",
    "async def run():\n",
    "    \"\"\"\n",
    "    This function sends water level data to an EventStream using Azure Event Hubs.\n",
    "    It fetches the current water level from a specified station and sends it as a JSON payload to the EventStream.\n",
    "    The function runs indefinitely, generating a new data point every 6 minutes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a producer client to send messages to the EventStream.\n",
    "    # Specify a connection string to your EventStream namespace and the eventstream name.\n",
    "    producer = EventHubProducerClient.from_connection_string(\n",
    "        conn_str=EVENTSTREAM_CONNECTION_STR, eventhub_name=EVENTSTREAM_NAME\n",
    "    )\n",
    "\n",
    "    # Specify the station ID\n",
    "    stationID = \"9445958\"  # Sample Bremerton, WA [9445958]\n",
    "\n",
    "    async with producer:\n",
    "        while True:\n",
    "            # Create a batch.\n",
    "            event_data_batch = await producer.create_batch()\n",
    "\n",
    "            # Fetch current water level in feet\n",
    "            station_data = fetch_water_level(stationID)\n",
    "           \n",
    "            if station_data is not None:\n",
    "                # Fetch station location\n",
    "                data = station_data[\"data\"]\n",
    "                if len(data) > 0:\n",
    "                    water_level = data[0][\"v\"]\n",
    "                station_latitude = station_data[\"metadata\"][\"lat\"]\n",
    "                station_longitude = station_data[\"metadata\"][\"lon\"]\n",
    "\n",
    "                # Create a data point with the time and value\n",
    "                current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                # Generate the data structure\n",
    "                water_point = {\n",
    "                    \"Current_time\": current_time,\n",
    "                    \"Water Level Value\": water_level,\n",
    "                    \"Station Latitude\": station_latitude,\n",
    "                    \"Station Longitude\": station_longitude,\n",
    "                    \"Station\": \"Water Station\",\n",
    "                }\n",
    "\n",
    "                # Convert the curve data to JSON format\n",
    "                json_water_data = json.dumps(water_point)\n",
    "                event_data_batch.add(EventData(json_water_data))\n",
    "\n",
    "                # Send the batch of events to the event hub.\n",
    "                await producer.send_batch(event_data_batch)\n",
    "\n",
    "            # Wait for 6 minutes before generating the next point\n",
    "            time.sleep(360)\n",
    "\n",
    "await run()"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "92aacb52-edda-4317-b36e-46667dd9aa29",
    "default_lakehouse_name": "FGILakeDB",
    "default_lakehouse_workspace_id": "babe6663-3785-460a-9b15-9cf81c0ba9e7",
    "known_lakehouses": [
     {
      "id": "92aacb52-edda-4317-b36e-46667dd9aa29"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "notebook_environment": {},
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    },
    "enableDebugMode": false
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
