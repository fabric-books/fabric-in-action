{"cells":[{"cell_type":"markdown","source":["# Loading data into the Silver zone from the Bronze zone"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"2d5ba8de-4d89-484a-9a75-2e82f3286d17"},{"cell_type":"markdown","source":["![image-alt-text](https://learn.microsoft.com/en-us/fabric/onelake/media/onelake-medallion-lakehouse-architecture/onelake-medallion-lakehouse-architecture-example.png)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5261e747-094c-4c70-b8d3-1ad08e07efce"},{"cell_type":"markdown","source":["## Loading the City dimension table\n","Using the PySpark libraries, we'll define the schema for the CSV files contained in the “wwi/full/dimension_city” directory. This explicit schema definition optimizes data loading performance. Data will be read from and written to a Lakehouse table in Delta Lake format."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"92e2d78d-2087-4128-bfe3-f572fab9a8cc"},{"cell_type":"code","source":["from pyspark.sql.types import *\n","\n","csv_schema = StructType(\n","    [\n","        StructField(    'CityKey'                   , IntegerType(), True), \n","        StructField(    'CityID'                    , IntegerType(), True), \n","        StructField(    'City'                      , StringType(), True), \n","        StructField(    'StateProvince'             , StringType(), True), \n","        StructField(    'Country'                   , StringType(), True), \n","        StructField(    'Continent'                 , StringType(), True), \n","        StructField(    'SalesTerritory'            , StringType(), True), \n","        StructField(    'Region'                    , StringType(), True), \n","        StructField(    'SubRegion'                 , StringType(), True), \n","        StructField(    'Location'                  , StringType(), True), \n","        StructField(    'LatestRecordedPopulation'  , LongType(), True), \n","        StructField(    'ValidFrom'                 , TimestampType(), True), \n","        StructField(    'ValidTo'                   , TimestampType(), True), \n","        StructField(    'LineageKey'                , IntegerType(), True)\n","    ]\n",")\n","\n","df = (\n","    spark.read.format(\"csv\")\n","    .schema(csv_schema)\n","    .option(\"header\", \"true\")\n","    .load(\"Files/wwi/full/dimension_city\")\n",")\n","\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/city\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"8b9b1327-1da8-474b-8880-91fd99c516c3","normalized_state":"finished","queued_time":"2025-05-13T12:27:54.8853576Z","session_start_time":"2025-05-13T12:27:54.8863334Z","execution_start_time":"2025-05-13T12:28:05.4827869Z","execution_finish_time":"2025-05-13T12:28:10.5577822Z","parent_msg_id":"44ea3d38-89c5-44ba-ba9b-9b7536b9bd1f"},"text/plain":"StatementMeta(, 8b9b1327-1da8-474b-8880-91fd99c516c3, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ab158a21-4602-43d6-bb23-0078db8e019f"},{"cell_type":"markdown","source":["## Loading the Date dimension table\n","Using the PySpark libraries, we're going to define the schema for the CSV files contained in the “wwi/full/dimension_date” directory. This explicit schema definition optimizes data loading performance. The data will be read from and written to a Lakehouse table in Delta Lake format."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"44b9d5b5-db5d-4e56-b1ab-96393f7d82cb"},{"cell_type":"code","source":["from pyspark.sql.types import *\n","\n","csv_schema = StructType(\n","    [\n","        StructField(    \"Date\"                  , TimestampType(), True),\n","        StructField(    \"DayNumber\"             , IntegerType(), True),\n","        StructField(    \"Day\"                   , StringType(), True),\n","        StructField(    \"MonthName\"             , StringType(), True),\n","        StructField(    \"ShortMonthName\"        , StringType(), True),\n","        StructField(    \"CYMonthNumber\"         , IntegerType(), True),\n","        StructField(    \"CYMonthLabel\"          , StringType(), True),\n","        StructField(    \"CYYear\"                , IntegerType(), True),\n","        StructField(    \"CYYearLabel\"           , StringType(), True),\n","        StructField(    \"FYMonthNumber\"         , IntegerType(), True),\n","        StructField(    \"FYMonthLabel\"          , StringType(), True),\n","        StructField(    \"FYYear\"                , IntegerType(), True),\n","        StructField(    \"FYYearLabel\"           , StringType(), True),\n","        StructField(    \"WeekNumber\"            , IntegerType(), True),\n","    ]\n",")\n","\n","df = (\n","    spark.read.format(\"csv\")\n","    .schema(csv_schema)\n","    .option(\"header\", \"true\")\n","    .load(\"Files/wwi/full/dimension_date\")\n",")\n","\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/date\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"8b9b1327-1da8-474b-8880-91fd99c516c3","normalized_state":"finished","queued_time":"2025-05-13T12:27:54.8884634Z","session_start_time":null,"execution_start_time":"2025-05-13T12:28:10.5599048Z","execution_finish_time":"2025-05-13T12:28:12.8664734Z","parent_msg_id":"5a03f800-c6ff-4e75-9de6-a11bcdb0444d"},"text/plain":"StatementMeta(, 8b9b1327-1da8-474b-8880-91fd99c516c3, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"715c7d20-047c-430d-943b-72079912615a"},{"cell_type":"markdown","source":["## Loading the Customer dimension table\n","Using the PySpark libraries, we'll define the schema for the CSV files contained in the “wwi/full/dimension_customer” directory. This explicit schema definition optimizes data loading performance. Data will be read from and written to a Lakehouse table in Delta Lake format."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"479f598f-139e-4c24-92e8-9596a353f5f7"},{"cell_type":"code","source":["from pyspark.sql.types import *\n","\n","csv_schema = StructType(\n","    [\n","        StructField(    \"CustomerKey\"       , IntegerType(), True),\n","        StructField(    \"CustomerID\"        , IntegerType(), True),\n","        StructField(    \"Customer\"          , StringType(), True),\n","        StructField(    \"BillToCustomer\"    , StringType(), True),\n","        StructField(    \"Category\"          , StringType(), True),\n","        StructField(    \"BuyingGroup\"       , StringType(), True),\n","        StructField(    \"PrimaryContact\"    , StringType(), True),\n","        StructField(    \"PostalCode\"        , StringType(), True),\n","        StructField(    \"ValidFrom\"         , TimestampType(), True),\n","        StructField(    \"ValidTo\"           , TimestampType(), True),\n","        StructField(    \"LineageKey\"        , IntegerType(), True),\n","    ]\n",")\n","\n","df = (\n","    spark.read.format(\"csv\")\n","    .schema(csv_schema)\n","    .option(\"header\", \"true\")\n","    .load(\"Files/wwi/full/dimension_customer\")\n",")\n","\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/customer\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"8b9b1327-1da8-474b-8880-91fd99c516c3","normalized_state":"finished","queued_time":"2025-05-13T12:27:54.8903898Z","session_start_time":null,"execution_start_time":"2025-05-13T12:28:12.8685728Z","execution_finish_time":"2025-05-13T12:28:15.2157169Z","parent_msg_id":"d74beeaf-921d-42ea-9a75-362f5ed5e2b4"},"text/plain":"StatementMeta(, 8b9b1327-1da8-474b-8880-91fd99c516c3, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5dfd5cfb-ba51-4266-8c35-afc6df94c86c"},{"cell_type":"markdown","source":["## Loading the Employee dimension table\n","Using the PySpark libraries, we're going to define the schema for the CSV files contained in the “wwi/full/dimension_employee” directory. This explicit schema definition optimizes data loading performance. Data will be read from and written to a Lakehouse table in Delta Lake format."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ae4063de-17f0-45be-9e46-6a4e051086ab"},{"cell_type":"code","source":["from pyspark.sql.types import *\n","\n","csv_schema = StructType(\n","    [\n","        StructField(    \"EmployeeKey\"       , IntegerType(), True),\n","        StructField(    \"EmployeeID\"        , IntegerType(), True),\n","        StructField(    \"EmployeeName\"      , StringType(), True),\n","        StructField(    \"PreferredName\"     , StringType(), True),\n","        StructField(    \"IsSalesPerson\"     , BooleanType(), True),\n","        StructField(    \"Photo\"             , StringType(), True),\n","        StructField(    \"ValidFrom\"         , TimestampType(), True),\n","        StructField(    \"ValidTo\"           , TimestampType(), True),\n","        StructField(    \"LineageKey\"        , IntegerType(), True),\n","    ]\n",")\n","\n","df = (\n","    spark.read.format(\"csv\")\n","    .schema(csv_schema)\n","    .option(\"header\", \"true\")\n","    .load(\"Files/wwi/full/dimension_employee\")\n",")\n","\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/employee\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"8b9b1327-1da8-474b-8880-91fd99c516c3","normalized_state":"finished","queued_time":"2025-05-13T12:27:54.8924408Z","session_start_time":null,"execution_start_time":"2025-05-13T12:28:15.2177536Z","execution_finish_time":"2025-05-13T12:28:17.495742Z","parent_msg_id":"ee28e2cc-db73-41bc-a52a-622655044b58"},"text/plain":"StatementMeta(, 8b9b1327-1da8-474b-8880-91fd99c516c3, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"580c885c-eebb-473a-a865-338e9687f2ae"},{"cell_type":"markdown","source":["## Loading the StockItem dimension table\n","Using the PySpark libraries, we're going to define the schema for the CSV files contained in the “wwi/full/dimension_stock_item” directory. This explicit schema definition optimizes data loading performance. Data will be read from and written to a Lakehouse table in Delta Lake format."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6c88b899-4a5a-40ca-b746-c70c51751345"},{"cell_type":"code","source":["from pyspark.sql.types import *\n","\n","csv_schema = StructType(\n","    [\n","        StructField(    \"StockItemKey\"              , IntegerType(), True),\n","        StructField(    \"StockItemID\"               , IntegerType(), True),\n","        StructField(    \"StockItem\"                 , StringType(), True),\n","        StructField(    \"Color\"                     , StringType(), True),\n","        StructField(    \"SellingPackage\"            , StringType(), True),\n","        StructField(    \"BuyingPackage\"             , StringType(), True),\n","        StructField(    \"Brand\"                     , StringType(), True),\n","        StructField(    \"Size\"                      , StringType(), True),\n","        StructField(    \"LeadTimeDays\"              , IntegerType(), True),\n","        StructField(    \"QuantityPerOuter\"          , IntegerType(), True),\n","        StructField(    \"IsChillerStock\"            , BooleanType(), True),\n","        StructField(    \"Barcode\"                   , StringType(), True),\n","        StructField(    \"TaxRate\"                   , DecimalType(18, 2), True),\n","        StructField(    \"UnitPrice\"                 , DecimalType(18, 2), True),\n","        StructField(    \"RecommendedRetailPrice\"    , DecimalType(18, 2), True),\n","        StructField(    \"WeightPerUnit\"             , DecimalType(18, 2), True),\n","        StructField(    \"Photo\"                     , StringType(), True),\n","        StructField(    \"ValidFrom\"                 , TimestampType(), True),\n","        StructField(    \"ValidTo\"                   , TimestampType(), True),\n","        StructField(    \"LineageKey\"                , IntegerType(), True),\n","    ]   \n",")\n","\n","df = (\n","    spark.read.format(\"csv\")\n","    .schema(csv_schema)\n","    .option(\"header\", \"true\")\n","    .load(\"Files/wwi/full/dimension_stock_item\")\n",")\n","\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/stockitem\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"8b9b1327-1da8-474b-8880-91fd99c516c3","normalized_state":"finished","queued_time":"2025-05-13T12:27:54.8943911Z","session_start_time":null,"execution_start_time":"2025-05-13T12:28:17.497867Z","execution_finish_time":"2025-05-13T12:28:19.822069Z","parent_msg_id":"afecae1b-1813-4fa7-912a-430d9ad25c28"},"text/plain":"StatementMeta(, 8b9b1327-1da8-474b-8880-91fd99c516c3, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"998b900c-3270-4aab-9474-4773f05873cd"},{"cell_type":"markdown","source":["## Loading the Sales fact table\n","Using the PySpark libraries, we're going to define the schema for the CSV files contained in the “wwi/full/dimension_stock_item” directory. This explicit schema definition optimizes data loading performance. Data will be read from and written to a Lakehouse table in Delta Lake format."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6bc50327-0947-4aef-a00d-cedc33a1c17a"},{"cell_type":"code","source":["from pyspark.sql.types import *\n","from pyspark.sql.functions import col, year, month, quarter\n","\n","csv_schema = StructType(\n","    [\n","        StructField(    \"SaleKey\"               , LongType(), True),\n","        StructField(    \"CityKey\"               , IntegerType(), True),\n","        StructField(    \"CustomerKey\"           , IntegerType(), True),\n","        StructField(    \"BillToCustomerKey\"     , IntegerType(), True),\n","        StructField(    \"StockItemKey\"          , IntegerType(), True),\n","        StructField(    \"InvoiceDateKey\"        , TimestampType(), True),\n","        StructField(    \"DeliveryDateKey\"       , TimestampType(), True),\n","        StructField(    \"SalespersonKey\"        , IntegerType(), True),\n","        StructField(    \"InvoiceID\"             , IntegerType(), True),\n","        StructField(    \"Description\"           , StringType(), True),\n","        StructField(    \"Package\"               , StringType(), True),\n","        StructField(    \"Quantity\"              , IntegerType(), True),\n","        StructField(    \"UnitPrice\"             , DecimalType(18, 2), True),\n","        StructField(    \"TaxRate\"               , DecimalType(18, 2), True),\n","        StructField(    \"TotalExcludingTax\"     , DecimalType(18, 2), True),\n","        StructField(    \"TaxAmount\"             , DecimalType(18, 2), True),\n","        StructField(    \"Profit\"                , DecimalType(18, 2), True),\n","        StructField(    \"TotalIncludingTax\"     , DecimalType(18, 2), True),\n","        StructField(    \"TotalDryItems\"         , IntegerType(), True),\n","        StructField(    \"TotalChillerItems\"     , IntegerType(), True),\n","        StructField(    \"LineageKey\"            , IntegerType(), True),\n","    ]\n",")\n","\n","df = (\n","    spark.read.format(\"csv\")\n","    .schema(csv_schema)\n","    .option(\"header\", \"true\")\n","    .load(\"Files/wwi/full/fact_sale\")\n",")\n","\n","df = df.withColumn(\"Year\", year(col(\"InvoiceDateKey\")))\n","df = df.withColumn(\"Quarter\", quarter(col(\"InvoiceDateKey\")))\n","df = df.withColumn(\"Month\", month(col(\"InvoiceDateKey\")))\n","\n","df.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\", \"Quarter\").save(\"Tables/sales\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"8b9b1327-1da8-474b-8880-91fd99c516c3","normalized_state":"finished","queued_time":"2025-05-13T12:27:54.8961925Z","session_start_time":null,"execution_start_time":"2025-05-13T12:28:19.824191Z","execution_finish_time":"2025-05-13T12:30:50.0540348Z","parent_msg_id":"fc0b13a4-9f1f-4b93-af7d-1fe3bcf2d94d"},"text/plain":"StatementMeta(, 8b9b1327-1da8-474b-8880-91fd99c516c3, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7481e33f-4ebc-4355-82a4-bee0f47823be"},{"cell_type":"code","source":["%%sql\n","SELECT Year, Quarter, Month, count(1) AS Sales\n","FROM lakehouse_silver.sales\n","GROUP BY Year, Quarter, Month"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"8b9b1327-1da8-474b-8880-91fd99c516c3","normalized_state":"finished","queued_time":"2025-05-13T12:27:54.898007Z","session_start_time":null,"execution_start_time":"2025-05-13T12:30:50.0562005Z","execution_finish_time":"2025-05-13T12:31:10.5646252Z","parent_msg_id":"08cfcb50-be9c-47ea-be93-48dcad9217ec"},"text/plain":"StatementMeta(, 8b9b1327-1da8-474b-8880-91fd99c516c3, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":7,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"Year","type":"integer","nullable":true,"metadata":{}},{"name":"Quarter","type":"integer","nullable":true,"metadata":{}},{"name":"Month","type":"integer","nullable":true,"metadata":{}},{"name":"Sales","type":"long","nullable":false,"metadata":{}}]},"data":[[2000,1,3,"4645578"],[2000,1,2,"4329724"],[2000,2,6,"4501147"],[2000,3,9,"4467225"],[2000,1,1,"4650551"],[2000,3,7,"4658230"],[2000,3,8,"4636981"],[2000,2,5,"4654366"],[2000,2,4,"4490042"],[2000,4,10,"4602381"],[2000,4,11,"4514618"]]},"text/plain":"<Spark SQL result set with 11 rows and 4 fields>"},"metadata":{}}],"execution_count":7,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"afb94a19-e840-4758-8ffe-3efca02e1947"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"language_group":"synapse_pyspark"},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{},"synapse_widget":{"state":{},"version":"0.1"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"},"enableDebugMode":false}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}